{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ae0334-5eeb-4062-b380-a42acd260608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install shiba shiba-model evaluate datasets wandb arabert  accelerate -U nltk torchmetrics==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78bcf5d-aded-4be5-b93a-ed7580614867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import HfArgumentParser, Trainer, EvalPrediction\n",
    "\n",
    "from shiba import ShibaForClassification, CodepointTokenizer\n",
    "from training.helpers import DataArguments, get_base_shiba_state_dict,get_model_hyperparams, ShibaClassificationArgs, \\\n",
    "    ClassificationDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98999aa3-2e36-49cc-b380-3a2cd7c5687b",
   "metadata": {},
   "source": [
    "<h1> Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f7f657-65f9-4d7a-b3af-54c8ecf28d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '../checkpoint-611960.pt'\n",
    "seg_enable = True\n",
    "bert_model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "apply_farasa=False\n",
    "file_save = 'Submit_64_05'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06150a6e-4b6f-47a0-9786-e4fb7dab1d85",
   "metadata": {},
   "source": [
    "# Pre-Process data ( if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5035ac5-db7f-4138-ac0c-cc3ab27ec755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5a1f8-a559-4a22-9547-bdd1a896c234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e80a6f-e0f3-43b9-a106-7bca91aa232e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbdb87b-ae16-4cde-9a91-3597e505ee2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bd7db-27a7-42ef-bcf8-b978b76f606a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffee9d5-9443-40cc-9b63-5fb870ca46d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48643012-7bfb-41f2-89c0-56bf079c5900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49dcc4b9-1cc7-4e3a-88d6-9a7c9928103d",
   "metadata": {},
   "source": [
    "<h1> Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e52536-f22f-448e-860a-4f6b208fd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories :  {0: 'TUN', 1: 'BAG', 2: 'ALG', 3: 'DAM', 4: 'TRI', 5: 'ASW', 6: 'RAB', 7: 'SAN', 8: 'SFX', 9: 'BAS', 10: 'MOS', 11: 'JER', 12: 'MUS', 13: 'CAI', 14: 'RIY', 15: 'KHA', 16: 'BEN', 17: 'FES', 18: 'DOH', 19: 'SAL', 20: 'AMM', 21: 'ALE', 22: 'MSA', 23: 'ALX', 24: 'JED', 25: 'BEI'}\n",
      "id_by_category :  {'TUN': 0, 'BAG': 1, 'ALG': 2, 'DAM': 3, 'TRI': 4, 'ASW': 5, 'RAB': 6, 'SAN': 7, 'SFX': 8, 'BAS': 9, 'MOS': 10, 'JER': 11, 'MUS': 12, 'CAI': 13, 'RIY': 14, 'KHA': 15, 'BEN': 16, 'FES': 17, 'DOH': 18, 'SAL': 19, 'AMM': 20, 'ALE': 21, 'MSA': 22, 'ALX': 23, 'JED': 24, 'BEI': 25}\n"
     ]
    }
   ],
   "source": [
    "transformers.logging.set_verbosity_info()\n",
    "device = \"cuda\"\n",
    "parser = HfArgumentParser((ShibaClassificationArgs, DataArguments))\n",
    "\n",
    "prediction_label = 'label'\n",
    "\n",
    "df_train = pd.read_csv(\"data/MADAR-Corpus-26-train.tsv\",names=['Text', 'label'], sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"data/MADAR-Corpus-26-dev.tsv\",names=['Text', 'label'], sep=\"\\t\")\n",
    "df_testOrignal = pd.read_csv(\"data/MADAR-Corpus-26-test.tsv\",names=['Text', 'label'], sep=\"\\t\")\n",
    "\n",
    "categories = {idx: cat_name for idx, cat_name in enumerate(set(df_train[prediction_label]))}\n",
    "id_by_category = {val: key for key, val in categories.items()}\n",
    "\n",
    "print(\"categories : \", categories)\n",
    "print(\"id_by_category : \", id_by_category)\n",
    "df_train = Dataset.from_pandas(df_train)\n",
    "df_dev = Dataset.from_pandas(df_dev)\n",
    "df_test = Dataset.from_pandas(df_testOrignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2b9aa1-68f9-466d-8794-24b7c6ffcbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if seg_enable:\n",
    "    from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "    arabert_prep = ArabertPreprocessor(model_name=bert_model_name,apply_farasa_segmentation=apply_farasa)\n",
    "    # arabert_prep.preprocess()\n",
    "    df_train = pd.DataFrame(df_train)\n",
    "    df_test =  pd.DataFrame(df_test)\n",
    "    df_dev = pd.DataFrame(df_dev)\n",
    "    df_train['Text'] =  df_train['Text'].apply(arabert_prep.preprocess)\n",
    "    df_test['Text'] =  df_test['Text'].apply(arabert_prep.preprocess)\n",
    "    df_dev['Text']=df_dev['Text'].apply(arabert_prep.preprocess)\n",
    "    df_train = Dataset.from_pandas(df_train)\n",
    "    df_test = Dataset.from_pandas(df_test)\n",
    "    df_dev = Dataset.from_pandas(df_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c902096b-032e-48a7-9585-eeea46b81f30",
   "metadata": {},
   "source": [
    "<h1> Check files count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "512821a6-c6ab-42f2-a81b-6f71b05324a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41600, 5200, 5200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_dev), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c1538-8fe9-4290-a163-ad203d6f3960",
   "metadata": {},
   "source": [
    "<h1> Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96084fc7-90db-4c9e-8ca3-bda9727a8f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout': 0.1, 'deep_transformer_stack_layers': 12, 'local_attention_window': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and using base shiba states from ../checkpoint-611960.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CodepointTokenizer()\n",
    "model_hyperparams = {'dropout': 0.1, 'deep_transformer_stack_layers': 12, 'local_attention_window': 128}\n",
    "print(model_hyperparams)\n",
    "model = ShibaForClassification(vocab_size=len(categories), **model_hyperparams)\n",
    "data_collator = ClassificationDataCollator()\n",
    "print('Loading and using base shiba states from', model_path)\n",
    "checkpoint_state_dict = torch.load(model_path)\n",
    "model.shiba_model.load_state_dict(get_base_shiba_state_dict(checkpoint_state_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f545d-1e7d-42b9-afbc-b58afc95d016",
   "metadata": {},
   "source": [
    "<h1>Input IDs Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04750222-c969-4863-8139-843b163be456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_example(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Text'])['input_ids'][:model.config.max_length],\n",
    "        'labels': id_by_category[example[prediction_label]]\n",
    "    }\n",
    "def process_exampleTemp(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Text'])['input_ids'][:model.config.max_length],\n",
    "        'labels': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952ccbe-9bfa-43d2-8036-dd3d64f280e3",
   "metadata": {},
   "source": [
    "<h1> Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "997544c5-69cb-402b-a67f-3aeb2844e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred: EvalPrediction) -> Dict:\n",
    "    try:\n",
    "        # Convert predictions and labels to PyTorch tensors\n",
    "        # label_probs = torch.tensor(pred.predictions)\n",
    "        label_probs, embeddings = pred.predictions\n",
    "        labels = torch.tensor(pred.label_ids)\n",
    "        label_probs = torch.exp(torch.tensor(label_probs))  # undo the log in log softmax, get indices\n",
    "        # # Compute accuracy\n",
    "        # accuracy = torchmetrics.functional.accuracy(label_probs, labels, num_classes=len(categories))\n",
    "\n",
    "        # Compute F1 score\n",
    "        f1_score = torchmetrics.functional.f1(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # Compute recall\n",
    "        recall = torchmetrics.functional.recall(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # Compute precision\n",
    "        precision = torchmetrics.functional.precision(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # print(\"label_probs : \", label_probs, \" labels : \", labels)\n",
    "\n",
    "        metrics = {\n",
    "            # 'accuracy': accuracy.item(),\n",
    "            'f1_score': f1_score.item(),\n",
    "            'recall': recall.item(),\n",
    "            'precision': precision.item()\n",
    "        }\n",
    "\n",
    "        # print(\"metrics : \", metrics)\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        return metrics\n",
    "    except:\n",
    "        print(\"pred : \", pred)\n",
    "        print(\"pred.predictions : \", pred.predictions)\n",
    "        print(\"label_probs : \", label_probs)\n",
    "        print(\"label_probs.size : \", label_probs.size())\n",
    "        print(\"labels : \", labels)\n",
    "        print(\"labels.size() : \", labels.size())\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9d59d-5bcd-4308-89a7-54a37e87c062",
   "metadata": {},
   "source": [
    "<h1> Fine-tune args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec5c238-c697-4d4b-95c3-966cac0831e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = ShibaClassificationArgs(\n",
    "    per_device_eval_batch_size=64,\n",
    "    per_device_train_batch_size=64,\n",
    "    data_seed=42,\n",
    "    seed=42,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    do_train=True,\n",
    "    dropout=0.5,\n",
    "    eval_accumulation_steps=None,\n",
    "    eval_delay=0,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    output_dir=\"fine_result\",\n",
    "    prediction_loss_only=False,\n",
    "    report_to=[],\n",
    "    run_name=\"fine_result\",\n",
    "    save_strategy='no',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c11aa-5324-46e6-95c7-2873f311f6d4",
   "metadata": {},
   "source": [
    "<h1> Setup the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9e1b475-99a5-454b-bea6-1d28ad0f9e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5818942cbe1849218b55d9d33a1cfb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7cf279ee264ae29084cf3ffefff092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# print(all_data)\n",
    "trainer = Trainer(model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=df_train.map(process_example, remove_columns=list(df_train[0].keys())),\n",
    "                eval_dataset=df_dev.map(process_example, remove_columns=list(df_dev[0].keys())),\n",
    "                compute_metrics=compute_metrics,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20add24d-64a0-4ff8-83f4-6bd7bf005fc1",
   "metadata": {},
   "source": [
    "<h1> Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eca7ad8d-8080-43e8-996f-83d4444e5d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 41,600\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6,500\n",
      "  Number of trainable parameters = 120,785,690\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6500' max='6500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6500/6500 17:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.258200</td>\n",
       "      <td>3.247994</td>\n",
       "      <td>0.041043</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>0.070819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.185600</td>\n",
       "      <td>3.013060</td>\n",
       "      <td>0.099585</td>\n",
       "      <td>0.131538</td>\n",
       "      <td>0.159810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.859800</td>\n",
       "      <td>2.510997</td>\n",
       "      <td>0.212664</td>\n",
       "      <td>0.255769</td>\n",
       "      <td>0.243656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.541100</td>\n",
       "      <td>2.280811</td>\n",
       "      <td>0.267535</td>\n",
       "      <td>0.299423</td>\n",
       "      <td>0.292639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.359100</td>\n",
       "      <td>2.096139</td>\n",
       "      <td>0.304274</td>\n",
       "      <td>0.333269</td>\n",
       "      <td>0.341267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.220900</td>\n",
       "      <td>2.015607</td>\n",
       "      <td>0.337418</td>\n",
       "      <td>0.358654</td>\n",
       "      <td>0.367359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.056800</td>\n",
       "      <td>1.898062</td>\n",
       "      <td>0.366933</td>\n",
       "      <td>0.386923</td>\n",
       "      <td>0.380928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.963900</td>\n",
       "      <td>1.827565</td>\n",
       "      <td>0.392499</td>\n",
       "      <td>0.408462</td>\n",
       "      <td>0.406971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.881700</td>\n",
       "      <td>1.795691</td>\n",
       "      <td>0.397490</td>\n",
       "      <td>0.416154</td>\n",
       "      <td>0.430091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.842000</td>\n",
       "      <td>1.711817</td>\n",
       "      <td>0.414849</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>0.434427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.807800</td>\n",
       "      <td>1.711396</td>\n",
       "      <td>0.427392</td>\n",
       "      <td>0.441346</td>\n",
       "      <td>0.446854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.774300</td>\n",
       "      <td>1.663007</td>\n",
       "      <td>0.443830</td>\n",
       "      <td>0.458077</td>\n",
       "      <td>0.469125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.774800</td>\n",
       "      <td>1.631899</td>\n",
       "      <td>0.463716</td>\n",
       "      <td>0.471923</td>\n",
       "      <td>0.487110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.603300</td>\n",
       "      <td>1.594924</td>\n",
       "      <td>0.465746</td>\n",
       "      <td>0.473269</td>\n",
       "      <td>0.481186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.524200</td>\n",
       "      <td>1.583584</td>\n",
       "      <td>0.462950</td>\n",
       "      <td>0.477692</td>\n",
       "      <td>0.481550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.532900</td>\n",
       "      <td>1.572268</td>\n",
       "      <td>0.467726</td>\n",
       "      <td>0.478077</td>\n",
       "      <td>0.486578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.569300</td>\n",
       "      <td>1.559415</td>\n",
       "      <td>0.484696</td>\n",
       "      <td>0.490769</td>\n",
       "      <td>0.504630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.523500</td>\n",
       "      <td>1.536552</td>\n",
       "      <td>0.485829</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.503960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.486000</td>\n",
       "      <td>1.515849</td>\n",
       "      <td>0.491681</td>\n",
       "      <td>0.500192</td>\n",
       "      <td>0.511142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.404500</td>\n",
       "      <td>1.511686</td>\n",
       "      <td>0.508940</td>\n",
       "      <td>0.512692</td>\n",
       "      <td>0.515697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.340300</td>\n",
       "      <td>1.495759</td>\n",
       "      <td>0.503990</td>\n",
       "      <td>0.510385</td>\n",
       "      <td>0.530078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.340600</td>\n",
       "      <td>1.493822</td>\n",
       "      <td>0.505362</td>\n",
       "      <td>0.513077</td>\n",
       "      <td>0.512715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.360300</td>\n",
       "      <td>1.493401</td>\n",
       "      <td>0.507861</td>\n",
       "      <td>0.517115</td>\n",
       "      <td>0.525827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.351000</td>\n",
       "      <td>1.465098</td>\n",
       "      <td>0.518722</td>\n",
       "      <td>0.522885</td>\n",
       "      <td>0.534011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.338900</td>\n",
       "      <td>1.465853</td>\n",
       "      <td>0.520941</td>\n",
       "      <td>0.525962</td>\n",
       "      <td>0.529244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.306300</td>\n",
       "      <td>1.441353</td>\n",
       "      <td>0.521137</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.536725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.221200</td>\n",
       "      <td>1.487813</td>\n",
       "      <td>0.529278</td>\n",
       "      <td>0.531154</td>\n",
       "      <td>0.546030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.172100</td>\n",
       "      <td>1.455933</td>\n",
       "      <td>0.522173</td>\n",
       "      <td>0.528462</td>\n",
       "      <td>0.542401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.228100</td>\n",
       "      <td>1.438968</td>\n",
       "      <td>0.535870</td>\n",
       "      <td>0.537885</td>\n",
       "      <td>0.546498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.176900</td>\n",
       "      <td>1.463112</td>\n",
       "      <td>0.537713</td>\n",
       "      <td>0.541538</td>\n",
       "      <td>0.547140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.175900</td>\n",
       "      <td>1.443862</td>\n",
       "      <td>0.540703</td>\n",
       "      <td>0.542115</td>\n",
       "      <td>0.553770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.185700</td>\n",
       "      <td>1.438029</td>\n",
       "      <td>0.536545</td>\n",
       "      <td>0.539231</td>\n",
       "      <td>0.551173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.120400</td>\n",
       "      <td>1.441186</td>\n",
       "      <td>0.541760</td>\n",
       "      <td>0.544038</td>\n",
       "      <td>0.554222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.072500</td>\n",
       "      <td>1.442347</td>\n",
       "      <td>0.547140</td>\n",
       "      <td>0.548654</td>\n",
       "      <td>0.559455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.055400</td>\n",
       "      <td>1.473681</td>\n",
       "      <td>0.539266</td>\n",
       "      <td>0.542885</td>\n",
       "      <td>0.561183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.079200</td>\n",
       "      <td>1.429970</td>\n",
       "      <td>0.546204</td>\n",
       "      <td>0.549615</td>\n",
       "      <td>0.558133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.082700</td>\n",
       "      <td>1.431801</td>\n",
       "      <td>0.557732</td>\n",
       "      <td>0.559423</td>\n",
       "      <td>0.566151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.053400</td>\n",
       "      <td>1.439810</td>\n",
       "      <td>0.552503</td>\n",
       "      <td>0.554808</td>\n",
       "      <td>0.560383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.098000</td>\n",
       "      <td>1.423544</td>\n",
       "      <td>0.553439</td>\n",
       "      <td>0.555192</td>\n",
       "      <td>0.560622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.993300</td>\n",
       "      <td>1.457758</td>\n",
       "      <td>0.559115</td>\n",
       "      <td>0.559231</td>\n",
       "      <td>0.571267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.966800</td>\n",
       "      <td>1.453997</td>\n",
       "      <td>0.558080</td>\n",
       "      <td>0.558462</td>\n",
       "      <td>0.566303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.991000</td>\n",
       "      <td>1.436417</td>\n",
       "      <td>0.558013</td>\n",
       "      <td>0.558846</td>\n",
       "      <td>0.566330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.990900</td>\n",
       "      <td>1.434545</td>\n",
       "      <td>0.554566</td>\n",
       "      <td>0.557885</td>\n",
       "      <td>0.561663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>1.442835</td>\n",
       "      <td>0.560212</td>\n",
       "      <td>0.560577</td>\n",
       "      <td>0.570584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.989700</td>\n",
       "      <td>1.433570</td>\n",
       "      <td>0.558279</td>\n",
       "      <td>0.558462</td>\n",
       "      <td>0.571110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.939900</td>\n",
       "      <td>1.435297</td>\n",
       "      <td>0.567052</td>\n",
       "      <td>0.565385</td>\n",
       "      <td>0.576746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>1.445103</td>\n",
       "      <td>0.560953</td>\n",
       "      <td>0.561154</td>\n",
       "      <td>0.571279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>1.444885</td>\n",
       "      <td>0.561565</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.569408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.918700</td>\n",
       "      <td>1.455180</td>\n",
       "      <td>0.560930</td>\n",
       "      <td>0.561346</td>\n",
       "      <td>0.571334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.933700</td>\n",
       "      <td>1.437063</td>\n",
       "      <td>0.566552</td>\n",
       "      <td>0.566731</td>\n",
       "      <td>0.574075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.916600</td>\n",
       "      <td>1.454447</td>\n",
       "      <td>0.563467</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.571752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>1.443327</td>\n",
       "      <td>0.570494</td>\n",
       "      <td>0.571346</td>\n",
       "      <td>0.578253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.841900</td>\n",
       "      <td>1.451670</td>\n",
       "      <td>0.570630</td>\n",
       "      <td>0.570577</td>\n",
       "      <td>0.576584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.859500</td>\n",
       "      <td>1.455230</td>\n",
       "      <td>0.568059</td>\n",
       "      <td>0.568654</td>\n",
       "      <td>0.575579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>1.453318</td>\n",
       "      <td>0.569804</td>\n",
       "      <td>0.571346</td>\n",
       "      <td>0.575791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>1.453942</td>\n",
       "      <td>0.567712</td>\n",
       "      <td>0.567692</td>\n",
       "      <td>0.573505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.878700</td>\n",
       "      <td>1.454703</td>\n",
       "      <td>0.567729</td>\n",
       "      <td>0.567308</td>\n",
       "      <td>0.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.874500</td>\n",
       "      <td>1.446452</td>\n",
       "      <td>0.568268</td>\n",
       "      <td>0.568654</td>\n",
       "      <td>0.575859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.845500</td>\n",
       "      <td>1.451758</td>\n",
       "      <td>0.569182</td>\n",
       "      <td>0.569808</td>\n",
       "      <td>0.576289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.828500</td>\n",
       "      <td>1.447931</td>\n",
       "      <td>0.572538</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.580172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.807700</td>\n",
       "      <td>1.460654</td>\n",
       "      <td>0.571099</td>\n",
       "      <td>0.571731</td>\n",
       "      <td>0.577675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.830800</td>\n",
       "      <td>1.455573</td>\n",
       "      <td>0.569261</td>\n",
       "      <td>0.569808</td>\n",
       "      <td>0.575831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.824300</td>\n",
       "      <td>1.455745</td>\n",
       "      <td>0.571777</td>\n",
       "      <td>0.571539</td>\n",
       "      <td>0.580278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.812600</td>\n",
       "      <td>1.450310</td>\n",
       "      <td>0.570989</td>\n",
       "      <td>0.570962</td>\n",
       "      <td>0.577567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.822900</td>\n",
       "      <td>1.450320</td>\n",
       "      <td>0.571912</td>\n",
       "      <td>0.571923</td>\n",
       "      <td>0.577815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad8bc7-519b-44d1-b6bb-98b3ab95c38d",
   "metadata": {},
   "source": [
    "<h1> Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9f2e88a-9647-4c44-a090-693b15b124df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_exampleTemp at 0x7fac08b55750> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e8ab9b091748a8bb566f44ce082f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(df_test.map(process_exampleTemp, remove_columns=list(df_test[0].keys())))\n",
    "df_testOrignal[prediction_label] = [categories[x] for x in np.argmax(pred.predictions[0], axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcb15f-9212-4e82-b73e-553b9c628b7e",
   "metadata": {},
   "source": [
    "<h1> Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17e1cbaf-6edb-42f9-b9a0-d59c0ca616db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSA\n",
       "0  ALE\n",
       "1  SAN\n",
       "2  AMM"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testOrignal[[prediction_label]].to_csv(file_save+'/madar.tsv', index=False, header=False, sep=\"\\t\")\n",
    "pd.read_csv(file_save+\"/madar.tsv\", sep=\"\\t\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2b710f-ef02-4917-9c06-adf6a12ddf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ffbe5f-5741-4fde-a68c-f62c1bb4028b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f123ce-cb47-4680-954e-1fd2bc8425b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
