{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ae0334-5eeb-4062-b380-a42acd260608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install shiba shiba-model evaluate datasets wandb arabert  accelerate -U nltk torchmetrics==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78bcf5d-aded-4be5-b93a-ed7580614867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import HfArgumentParser, Trainer, EvalPrediction\n",
    "\n",
    "from shiba import ShibaForClassification, CodepointTokenizer\n",
    "from training.helpers import DataArguments, get_base_shiba_state_dict,get_model_hyperparams, ShibaClassificationArgs, \\\n",
    "    ClassificationDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98999aa3-2e36-49cc-b380-3a2cd7c5687b",
   "metadata": {},
   "source": [
    "<h1> Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f7f657-65f9-4d7a-b3af-54c8ecf28d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '../checkpoint-611960.pt'\n",
    "seg_enable = True\n",
    "bert_model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "apply_farasa=False\n",
    "file_save = 'SEC'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ded7b5-bae3-45fa-84fa-85a2453d5990",
   "metadata": {},
   "source": [
    "# Pre-Process data ( if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcc4b9-1cc7-4e3a-88d6-9a7c9928103d",
   "metadata": {},
   "source": [
    "<h1> Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e52536-f22f-448e-860a-4f6b208fd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories :  {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
      "id_by_category :  {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n"
     ]
    }
   ],
   "source": [
    "transformers.logging.set_verbosity_info()\n",
    "device = \"cuda\"\n",
    "parser = HfArgumentParser((ShibaClassificationArgs, DataArguments))\n",
    "\n",
    "prediction_label = 'gold_label'\n",
    "\n",
    "df_train = pd.read_csv(\"data/arabic_train.tsv\", sep=\"\\t\")\n",
    "df_testOrignal = pd.read_csv(\"data/arabic_dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "categories = {idx: cat_name for idx, cat_name in enumerate(set(df_train[prediction_label]))}\n",
    "id_by_category = {val: key for key, val in categories.items()}\n",
    "\n",
    "print(\"categories : \", categories)\n",
    "print(\"id_by_category : \", id_by_category)\n",
    "df_train = Dataset.from_pandas(df_train)\n",
    "df_test = Dataset.from_pandas(df_testOrignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ce53b1-092c-43aa-9328-f8a9cf661929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if seg_enable:\n",
    "    from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "    arabert_prep = ArabertPreprocessor(model_name=bert_model_name,apply_farasa_segmentation=apply_farasa)\n",
    "    # arabert_prep.preprocess()\n",
    "    df_train = pd.DataFrame(df_train)\n",
    "    df_test =  pd.DataFrame(df_test)\n",
    "\n",
    "    df_train['sentence1'] =  df_train['sentence1'].apply(arabert_prep.preprocess)\n",
    "    df_train['sentence2'] =  df_train['sentence2'].apply(arabert_prep.preprocess)\n",
    "    df_test['sentence1'] =  df_test['sentence1'].apply(arabert_prep.preprocess)\n",
    "    df_test['sentence2'] =  df_test['sentence2'].apply(arabert_prep.preprocess)\n",
    "\n",
    "    df_train = Dataset.from_pandas(df_train)\n",
    "    df_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c902096b-032e-48a7-9585-eeea46b81f30",
   "metadata": {},
   "source": [
    "<h1> Check files count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512821a6-c6ab-42f2-a81b-6f71b05324a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5010, 2490)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c1538-8fe9-4290-a163-ad203d6f3960",
   "metadata": {},
   "source": [
    "<h1> Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96084fc7-90db-4c9e-8ca3-bda9727a8f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout': 0.1, 'deep_transformer_stack_layers': 12, 'local_attention_window': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and using base shiba states from ../checkpoint-611960.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CodepointTokenizer()\n",
    "model_hyperparams = {'dropout': 0.1, 'deep_transformer_stack_layers': 12, 'local_attention_window': 128}\n",
    "print(model_hyperparams)\n",
    "model = ShibaForClassification(vocab_size=len(categories), **model_hyperparams)\n",
    "data_collator = ClassificationDataCollator()\n",
    "print('Loading and using base shiba states from', model_path)\n",
    "checkpoint_state_dict = torch.load(model_path)\n",
    "model.shiba_model.load_state_dict(get_base_shiba_state_dict(checkpoint_state_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f545d-1e7d-42b9-afbc-b58afc95d016",
   "metadata": {},
   "source": [
    "<h1>Input IDs Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04750222-c969-4863-8139-843b163be456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_example(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode([example['sentence1'], example['sentence2']])['input_ids'][:model.config.max_length],\n",
    "        'labels': id_by_category[example[prediction_label]]\n",
    "    }\n",
    "def process_exampleTemp(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode([example['sentence1'], example['sentence2']])['input_ids'][:model.config.max_length],\n",
    "        'labels': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952ccbe-9bfa-43d2-8036-dd3d64f280e3",
   "metadata": {},
   "source": [
    "<h1> Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "997544c5-69cb-402b-a67f-3aeb2844e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred: EvalPrediction) -> Dict:\n",
    "    try:\n",
    "        # Convert predictions and labels to PyTorch tensors\n",
    "        # label_probs = torch.tensor(pred.predictions)\n",
    "        label_probs, embeddings = pred.predictions\n",
    "        labels = torch.tensor(pred.label_ids)\n",
    "        label_probs = torch.exp(torch.tensor(label_probs))  # undo the log in log softmax, get indices\n",
    "        # # Compute accuracy\n",
    "        # accuracy = torchmetrics.functional.accuracy(label_probs, labels, num_classes=len(categories))\n",
    "\n",
    "        # Compute F1 score\n",
    "        f1_score = torchmetrics.functional.f1(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # Compute recall\n",
    "        recall = torchmetrics.functional.recall(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # Compute precision\n",
    "        precision = torchmetrics.functional.precision(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # print(\"label_probs : \", label_probs, \" labels : \", labels)\n",
    "\n",
    "        metrics = {\n",
    "            # 'accuracy': accuracy.item(),\n",
    "            'f1_score': f1_score.item(),\n",
    "            'recall': recall.item(),\n",
    "            'precision': precision.item()\n",
    "        }\n",
    "\n",
    "        # print(\"metrics : \", metrics)\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        return metrics\n",
    "    except:\n",
    "        print(\"pred : \", pred)\n",
    "        print(\"pred.predictions : \", pred.predictions)\n",
    "        print(\"label_probs : \", label_probs)\n",
    "        print(\"label_probs.size : \", label_probs.size())\n",
    "        print(\"labels : \", labels)\n",
    "        print(\"labels.size() : \", labels.size())\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9d59d-5bcd-4308-89a7-54a37e87c062",
   "metadata": {},
   "source": [
    "<h1> Fine-tune args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec5c238-c697-4d4b-95c3-966cac0831e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = ShibaClassificationArgs(\n",
    "    per_device_train_batch_size=16,\n",
    "    data_seed=42,\n",
    "    seed=42,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    do_train=True,\n",
    "    dropout=0.2,\n",
    "    evaluation_strategy='no',\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    output_dir=\"fine_result\",\n",
    "    prediction_loss_only=False,\n",
    "    report_to=[],\n",
    "    run_name=\"fine_result\",\n",
    "    save_strategy='no',\n",
    "    learning_rate=2e-07\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a143a9ce-1cdb-4b8a-b1ab-3a3315be03a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShibaClassificationArgs(output_dir='fine_result', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=12, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.025, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='fine_result/runs/Nov29_08-28-49_39ed78d26a6f', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=100, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.NO: 'no'>, save_steps=500, save_total_limit=None, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=300, dataloader_num_workers=0, past_index=-1, run_name='fine_result', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, masking_type='rand_span', load_only_model=False, dropout=0.2, deep_transformer_stack_layers=12, local_attention_window=128, trials=2, pretrained_bert=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c11aa-5324-46e6-95c7-2873f311f6d4",
   "metadata": {},
   "source": [
    "<h1> Setup the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d04dea7-ac0b-462e-8bf5-afdd662af4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if seg_enable:\n",
    "    compute_metrics=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9e1b475-99a5-454b-bea6-1d28ad0f9e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41be498ecc0e4c2cbf04744fb77d31f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5010 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309c863b419345c4aff56c740427a3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5010 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(all_data)\n",
    "trainer = Trainer(model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=df_train.map(process_example, remove_columns=list(df_train[0].keys())),\n",
    "                eval_dataset=df_train.map(process_example, remove_columns=list(df_train[0].keys())),\n",
    "                compute_metrics=compute_metrics,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20add24d-64a0-4ff8-83f4-6bd7bf005fc1",
   "metadata": {},
   "source": [
    "<h1> Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eca7ad8d-8080-43e8-996f-83d4444e5d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5,010\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,140\n",
      "  Number of trainable parameters = 120,768,003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3140' max='3140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3140/3140 05:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.103500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.851300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.707900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.594300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.181600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.126400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad8bc7-519b-44d1-b6bb-98b3ab95c38d",
   "metadata": {},
   "source": [
    "<h1> Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9f2e88a-9647-4c44-a090-693b15b124df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_exampleTemp at 0x7fae38e94ee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ed8b42beb74f888618b571b83dce7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2490\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(df_test.map(process_exampleTemp, remove_columns=list(df_test[0].keys())))\n",
    "df_testOrignal['prediction'] = [categories[x] for x in np.argmax(pred.predictions[0], axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcb15f-9212-4e82-b73e-553b9c628b7e",
   "metadata": {},
   "source": [
    "<h1> Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17e1cbaf-6edb-42f9-b9a0-d59c0ca616db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pairID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pairID     prediction\n",
       "0       1        neutral\n",
       "1       2  contradiction\n",
       "2       3        neutral"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testOrignal[['pairID', 'prediction']].to_csv(file_save+'/xnli.tsv', index=False, sep=\"\\t\")\n",
    "pd.read_csv(file_save+\"/xnli.tsv\", sep=\"\\t\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2b710f-ef02-4917-9c06-adf6a12ddf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data = pd.read_csv(\"data/diagnostic.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff262245-b2d0-4af4-ba4d-35cfbb4530e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if seg_enable:\n",
    "    diagnostic_data['sentence2'] =  diagnostic_data['sentence2'].apply(arabert_prep.preprocess)\n",
    "    diagnostic_data['sentence1'] =  diagnostic_data['sentence1'].apply(arabert_prep.preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eced81eb-5e9e-4c68-9bb4-b96e82c11ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0df2ec835649748de9827091685b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1147\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_diagnostic = Dataset.from_pandas(diagnostic_data)\n",
    "diagnostic_pred = trainer.predict(df_diagnostic.map(process_exampleTemp, remove_columns=list(df_diagnostic[0].keys())))\n",
    "diagnostic_data['prediction'] = [categories[x] for x in np.argmax(diagnostic_pred.predictions[0], axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56fa7f48-3878-4083-8276-df6bc29f12ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pairID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pairID     prediction\n",
       "0       0        neutral\n",
       "1       1        neutral\n",
       "2       2  contradiction"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostic_data[['pairID', 'prediction']].to_csv(file_save+'/diagnostic.tsv', index=False, sep=\"\\t\")\n",
    "pd.read_csv(file_save+\"/diagnostic.tsv\", sep=\"\\t\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a41994-58fe-4fe9-8292-c7ab657f6fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30aad9a-b2fc-4b1d-a282-63cde80a51ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
