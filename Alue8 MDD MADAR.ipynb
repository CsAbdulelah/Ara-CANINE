{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ae0334-5eeb-4062-b380-a42acd260608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install shiba shiba-model evaluate datasets wandb arabert  accelerate -U nltk torchmetrics==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78bcf5d-aded-4be5-b93a-ed7580614867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import HfArgumentParser, Trainer, EvalPrediction\n",
    "\n",
    "from shiba import ShibaForClassification, CodepointTokenizer\n",
    "from training.helpers import DataArguments, get_base_shiba_state_dict,get_model_hyperparams, ShibaClassificationArgs, \\\n",
    "    ClassificationDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98999aa3-2e36-49cc-b380-3a2cd7c5687b",
   "metadata": {},
   "source": [
    "<h1> Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f7f657-65f9-4d7a-b3af-54c8ecf28d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '../checkpoint-611960.pt'\n",
    "seg_enable = True\n",
    "bert_model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "apply_farasa=False\n",
    "file_save = 'Submit_8_02'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06150a6e-4b6f-47a0-9786-e4fb7dab1d85",
   "metadata": {},
   "source": [
    "# Pre-Process data ( if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5035ac5-db7f-4138-ac0c-cc3ab27ec755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5a1f8-a559-4a22-9547-bdd1a896c234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e80a6f-e0f3-43b9-a106-7bca91aa232e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbdb87b-ae16-4cde-9a91-3597e505ee2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bd7db-27a7-42ef-bcf8-b978b76f606a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffee9d5-9443-40cc-9b63-5fb870ca46d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48643012-7bfb-41f2-89c0-56bf079c5900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49dcc4b9-1cc7-4e3a-88d6-9a7c9928103d",
   "metadata": {},
   "source": [
    "<h1> Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e52536-f22f-448e-860a-4f6b208fd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories :  {0: 'SAL', 1: 'FES', 2: 'KHA', 3: 'BEI', 4: 'DAM', 5: 'ASW', 6: 'MOS', 7: 'TUN', 8: 'RIY', 9: 'BAG', 10: 'RAB', 11: 'ALG', 12: 'JER', 13: 'MUS', 14: 'AMM', 15: 'DOH', 16: 'SFX', 17: 'CAI', 18: 'BAS', 19: 'ALE', 20: 'BEN', 21: 'MSA', 22: 'TRI', 23: 'ALX', 24: 'SAN', 25: 'JED'}\n",
      "id_by_category :  {'SAL': 0, 'FES': 1, 'KHA': 2, 'BEI': 3, 'DAM': 4, 'ASW': 5, 'MOS': 6, 'TUN': 7, 'RIY': 8, 'BAG': 9, 'RAB': 10, 'ALG': 11, 'JER': 12, 'MUS': 13, 'AMM': 14, 'DOH': 15, 'SFX': 16, 'CAI': 17, 'BAS': 18, 'ALE': 19, 'BEN': 20, 'MSA': 21, 'TRI': 22, 'ALX': 23, 'SAN': 24, 'JED': 25}\n"
     ]
    }
   ],
   "source": [
    "transformers.logging.set_verbosity_info()\n",
    "device = \"cuda\"\n",
    "parser = HfArgumentParser((ShibaClassificationArgs, DataArguments))\n",
    "\n",
    "prediction_label = 'label'\n",
    "\n",
    "df_train = pd.read_csv(\"data/MADAR-Corpus-26-train.tsv\",names=['Text', 'label'], sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"data/MADAR-Corpus-26-dev.tsv\",names=['Text', 'label'], sep=\"\\t\")\n",
    "df_testOrignal = pd.read_csv(\"data/MADAR-Corpus-26-test.tsv\",names=['Text', 'label'], sep=\"\\t\")\n",
    "\n",
    "categories = {idx: cat_name for idx, cat_name in enumerate(set(df_train[prediction_label]))}\n",
    "id_by_category = {val: key for key, val in categories.items()}\n",
    "\n",
    "print(\"categories : \", categories)\n",
    "print(\"id_by_category : \", id_by_category)\n",
    "df_train = Dataset.from_pandas(df_train)\n",
    "df_dev = Dataset.from_pandas(df_dev)\n",
    "df_test = Dataset.from_pandas(df_testOrignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2b9aa1-68f9-466d-8794-24b7c6ffcbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if seg_enable:\n",
    "    from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "    arabert_prep = ArabertPreprocessor(model_name=bert_model_name,apply_farasa_segmentation=apply_farasa)\n",
    "    # arabert_prep.preprocess()\n",
    "    df_train = pd.DataFrame(df_train)\n",
    "    df_test =  pd.DataFrame(df_test)\n",
    "    df_dev = pd.DataFrame(df_dev)\n",
    "    df_train['Text'] =  df_train['Text'].apply(arabert_prep.preprocess)\n",
    "    df_test['Text'] =  df_test['Text'].apply(arabert_prep.preprocess)\n",
    "    df_dev['Text']=df_dev['Text'].apply(arabert_prep.preprocess)\n",
    "    df_train = Dataset.from_pandas(df_train)\n",
    "    df_test = Dataset.from_pandas(df_test)\n",
    "    df_dev = Dataset.from_pandas(df_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c902096b-032e-48a7-9585-eeea46b81f30",
   "metadata": {},
   "source": [
    "<h1> Check files count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "512821a6-c6ab-42f2-a81b-6f71b05324a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41600, 5200, 5200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_dev), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c1538-8fe9-4290-a163-ad203d6f3960",
   "metadata": {},
   "source": [
    "<h1> Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96084fc7-90db-4c9e-8ca3-bda9727a8f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout': 0.1, 'deep_transformer_stack_layers': 12, 'local_attention_window': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and using base shiba states from ../checkpoint-611960.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CodepointTokenizer()\n",
    "model_hyperparams = {'dropout': 0.1, 'deep_transformer_stack_layers': 12, 'local_attention_window': 128}\n",
    "print(model_hyperparams)\n",
    "model = ShibaForClassification(vocab_size=len(categories), **model_hyperparams)\n",
    "data_collator = ClassificationDataCollator()\n",
    "print('Loading and using base shiba states from', model_path)\n",
    "checkpoint_state_dict = torch.load(model_path)\n",
    "model.shiba_model.load_state_dict(get_base_shiba_state_dict(checkpoint_state_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f545d-1e7d-42b9-afbc-b58afc95d016",
   "metadata": {},
   "source": [
    "<h1>Input IDs Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04750222-c969-4863-8139-843b163be456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_example(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Text'])['input_ids'][:model.config.max_length],\n",
    "        'labels': id_by_category[example[prediction_label]]\n",
    "    }\n",
    "def process_exampleTemp(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Text'])['input_ids'][:model.config.max_length],\n",
    "        'labels': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952ccbe-9bfa-43d2-8036-dd3d64f280e3",
   "metadata": {},
   "source": [
    "<h1> Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "997544c5-69cb-402b-a67f-3aeb2844e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred: EvalPrediction) -> Dict:\n",
    "    try:\n",
    "        # Convert predictions and labels to PyTorch tensors\n",
    "        # label_probs = torch.tensor(pred.predictions)\n",
    "        label_probs, embeddings = pred.predictions\n",
    "        labels = torch.tensor(pred.label_ids)\n",
    "        label_probs = torch.exp(torch.tensor(label_probs))  # undo the log in log softmax, get indices\n",
    "        # # Compute accuracy\n",
    "        # accuracy = torchmetrics.functional.accuracy(label_probs, labels, num_classes=len(categories))\n",
    "\n",
    "        # Compute F1 score\n",
    "        f1_score = torchmetrics.functional.f1(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # Compute recall\n",
    "        recall = torchmetrics.functional.recall(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # Compute precision\n",
    "        precision = torchmetrics.functional.precision(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # print(\"label_probs : \", label_probs, \" labels : \", labels)\n",
    "\n",
    "        metrics = {\n",
    "            # 'accuracy': accuracy.item(),\n",
    "            'f1_score': f1_score.item(),\n",
    "            'recall': recall.item(),\n",
    "            'precision': precision.item()\n",
    "        }\n",
    "\n",
    "        # print(\"metrics : \", metrics)\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        return metrics\n",
    "    except:\n",
    "        print(\"pred : \", pred)\n",
    "        print(\"pred.predictions : \", pred.predictions)\n",
    "        print(\"label_probs : \", label_probs)\n",
    "        print(\"label_probs.size : \", label_probs.size())\n",
    "        print(\"labels : \", labels)\n",
    "        print(\"labels.size() : \", labels.size())\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9d59d-5bcd-4308-89a7-54a37e87c062",
   "metadata": {},
   "source": [
    "<h1> Fine-tune args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec5c238-c697-4d4b-95c3-966cac0831e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = ShibaClassificationArgs(\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    data_seed=42,\n",
    "    seed=42,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    do_train=True,\n",
    "    dropout=0.2,\n",
    "    eval_accumulation_steps=None,\n",
    "    eval_delay=0,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    output_dir=\"fine_result\",\n",
    "    prediction_loss_only=False,\n",
    "    report_to=[],\n",
    "    run_name=\"fine_result\",\n",
    "    save_strategy='no',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c11aa-5324-46e6-95c7-2873f311f6d4",
   "metadata": {},
   "source": [
    "<h1> Setup the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9e1b475-99a5-454b-bea6-1d28ad0f9e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79f4f59679d4c699c0fa411f270b68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576624cb942a4e8ebbe0bc1b595451f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# print(all_data)\n",
    "trainer = Trainer(model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=df_train.map(process_example, remove_columns=list(df_train[0].keys())),\n",
    "                eval_dataset=df_dev.map(process_example, remove_columns=list(df_dev[0].keys())),\n",
    "                compute_metrics=compute_metrics,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20add24d-64a0-4ff8-83f4-6bd7bf005fc1",
   "metadata": {},
   "source": [
    "<h1> Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7ad8d-8080-43e8-996f-83d4444e5d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 41,600\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 52,000\n",
      "  Number of trainable parameters = 120,785,690\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='52000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  201/52000 01:04 < 4:40:44, 3.08 it/s, Epoch 0.04/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.262900</td>\n",
       "      <td>3.259497</td>\n",
       "      <td>0.023093</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.034581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='317' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [317/650 00:16 < 00:17, 19.48 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5200\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "training = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad8bc7-519b-44d1-b6bb-98b3ab95c38d",
   "metadata": {},
   "source": [
    "<h1> Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2e88a-9647-4c44-a090-693b15b124df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(df_test.map(process_exampleTemp, remove_columns=list(df_test[0].keys())))\n",
    "df_testOrignal[prediction_label] = [categories[x] for x in np.argmax(pred.predictions[0], axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcb15f-9212-4e82-b73e-553b9c628b7e",
   "metadata": {},
   "source": [
    "<h1> Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1cbaf-6edb-42f9-b9a0-d59c0ca616db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_testOrignal[[prediction_label]].to_csv(file_save+'/madar.tsv', index=False, header=False, sep=\"\\t\")\n",
    "pd.read_csv(file_save+\"/madar.tsv\", sep=\"\\t\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b710f-ef02-4917-9c06-adf6a12ddf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7eecb5-1d7d-4184-9574-3354236560d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
