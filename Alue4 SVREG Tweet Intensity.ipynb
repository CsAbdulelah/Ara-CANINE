{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ae0334-5eeb-4062-b380-a42acd260608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install shiba shiba-model evaluate datasets wandb arabert  accelerate -U nltk torchmetrics==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78bcf5d-aded-4be5-b93a-ed7580614867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import HfArgumentParser, Trainer, EvalPrediction\n",
    "\n",
    "from shiba import ShibaForRegression, CodepointTokenizer\n",
    "from training.helpers import DataArguments, get_base_shiba_state_dict,get_model_hyperparams, ShibaClassificationArgs, \\\n",
    "    ClassificationDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98999aa3-2e36-49cc-b380-3a2cd7c5687b",
   "metadata": {},
   "source": [
    "<h1> Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37f7f657-65f9-4d7a-b3af-54c8ecf28d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '../checkpoint-611960.pt'\n",
    "seg_enable = True\n",
    "bert_model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "apply_farasa=False\n",
    "file_save = 'Submit_64_05'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcc4b9-1cc7-4e3a-88d6-9a7c9928103d",
   "metadata": {},
   "source": [
    "<h1> Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8e52536-f22f-448e-860a-4f6b208fd366",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_info()\n",
    "device = \"cuda\"\n",
    "parser = HfArgumentParser((ShibaClassificationArgs, DataArguments))\n",
    "\n",
    "prediction_label = 'Intensity Score'\n",
    "\n",
    "df_train = pd.read_csv(\"data/2018-Valence-reg-Ar-train.txt\", sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"data/2018-Valence-reg-Ar-dev.txt\", sep=\"\\t\")\n",
    "df_testOrignal = pd.read_csv(\"data/vreg_no_labels_v1.0.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_train = Dataset.from_pandas(df_train)\n",
    "df_dev = Dataset.from_pandas(df_dev)\n",
    "df_test = Dataset.from_pandas(df_testOrignal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c902096b-032e-48a7-9585-eeea46b81f30",
   "metadata": {},
   "source": [
    "<h1> Check files count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "512821a6-c6ab-42f2-a81b-6f71b05324a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(932, 138, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_dev), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c1615-166f-4714-bb98-9bee7668be2a",
   "metadata": {},
   "source": [
    "# Pre-Process data ( if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035e1ac-e28b-4dc1-892e-e6f7c9a005e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae224a97-e51f-4649-819a-c35eda0dd442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bc456-4b74-4d8d-ae96-fbb73af63652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9caebdd-714f-4a9a-bab6-b071378398c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if seg_enable:\n",
    "    from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "    arabert_prep = ArabertPreprocessor(model_name=bert_model_name,apply_farasa_segmentation=apply_farasa)\n",
    "    # arabert_prep.preprocess()\n",
    "    df_train = pd.DataFrame(df_train)\n",
    "    df_dev = pd.DataFrame(df_dev)\n",
    "    df_test = pd.DataFrame(df_test)\n",
    "    df_train['Tweet'] =  df_train['Tweet'].apply(arabert_prep.preprocess)\n",
    "    df_dev['Tweet'] =  df_dev['Tweet'].apply(arabert_prep.preprocess)\n",
    "    df_test['Tweet'] =  df_test['Tweet'].apply(arabert_prep.preprocess)\n",
    "    df_train = Dataset.from_pandas(df_train)\n",
    "    df_dev = Dataset.from_pandas(df_dev)\n",
    "    df_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c1538-8fe9-4290-a163-ad203d6f3960",
   "metadata": {},
   "source": [
    "<h1> Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96084fc7-90db-4c9e-8ca3-bda9727a8f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout': 0.1, 'deep_transformer_stack_layers': 12, 'local_attention_window': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and using base shiba states from ../checkpoint-611960.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CodepointTokenizer()\n",
    "model_hyperparams = {'dropout': 0.1, 'deep_transformer_stack_layers': 12, 'local_attention_window': 128}\n",
    "print(model_hyperparams)\n",
    "model = ShibaForRegression(**model_hyperparams)\n",
    "data_collator = ClassificationDataCollator()\n",
    "print('Loading and using base shiba states from', model_path)\n",
    "checkpoint_state_dict = torch.load(model_path)\n",
    "model.shiba_model.load_state_dict(get_base_shiba_state_dict(checkpoint_state_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f545d-1e7d-42b9-afbc-b58afc95d016",
   "metadata": {},
   "source": [
    "<h1>Input IDs Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04750222-c969-4863-8139-843b163be456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_example(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Tweet'])['input_ids'][:2048],\n",
    "        'labels': example[prediction_label]\n",
    "    }\n",
    "def process_exampleTemp(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Tweet'])['input_ids'][:2048],\n",
    "        'labels': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952ccbe-9bfa-43d2-8036-dd3d64f280e3",
   "metadata": {},
   "source": [
    "<h1> Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "997544c5-69cb-402b-a67f-3aeb2844e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred: EvalPrediction) -> Dict:\n",
    "    try:\n",
    "        # Convert predictions and labels to PyTorch tensors\n",
    "        # label_probs = torch.tensor(pred.predictions)\n",
    "        label_probs, embeddings = pred.predictions\n",
    "        \n",
    "        labels = torch.tensor(pred.label_ids)\n",
    "        label_probs = torch.tensor(label_probs).view(-1)\n",
    "        mse = torchmetrics.functional.mean_squared_error(label_probs, labels)\n",
    "        mee = torchmetrics.functional.mean_absolute_error(label_probs, labels)\n",
    "        \n",
    "\n",
    "        # print(\"label_probs : \", label_probs, \" labels : \", labels)\n",
    "\n",
    "        metrics = {\n",
    "            'mse': mse.item(),\n",
    "            'mee': mee.item(),\n",
    "        }\n",
    "\n",
    "        # print(\"metrics : \", metrics)\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        return metrics\n",
    "    except:\n",
    "        print(\"pred : \", pred)\n",
    "        print(\"pred.predictions : \", pred.predictions)\n",
    "        print(\"label_probs : \", label_probs)\n",
    "        print(\"labels : \", labels)\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9d59d-5bcd-4308-89a7-54a37e87c062",
   "metadata": {},
   "source": [
    "<h1> Fine-tune args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ec5c238-c697-4d4b-95c3-966cac0831e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = ShibaClassificationArgs(\n",
    "    per_device_eval_batch_size=64,\n",
    "    per_device_train_batch_size=64,\n",
    "    data_seed=42,\n",
    "    seed=42,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    do_train=True,\n",
    "    dropout=0.5,\n",
    "    eval_accumulation_steps=None,\n",
    "    eval_delay=0,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=10,\n",
    "    output_dir=\"fine_result\",\n",
    "    prediction_loss_only=False,\n",
    "    report_to=[],\n",
    "    run_name=\"fine_result\",\n",
    "    save_strategy='no',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c11aa-5324-46e6-95c7-2873f311f6d4",
   "metadata": {},
   "source": [
    "<h1> Setup the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d8459f1-0c0e-4e20-9fcc-cfe270b31154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if seg_enable:\n",
    "    compute_metrics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9e1b475-99a5-454b-bea6-1d28ad0f9e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7359bd92ffe94a99b4fa835d648b54d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/932 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990ab62e9c3a442b98326d61418d3570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# print(all_data)\n",
    "trainer = Trainer(model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=df_train.map(process_example, remove_columns=list(df_train[0].keys())),\n",
    "                eval_dataset=df_dev.map(process_example, remove_columns=list(df_dev[0].keys())),\n",
    "                compute_metrics=compute_metrics,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20add24d-64a0-4ff8-83f4-6bd7bf005fc1",
   "metadata": {},
   "source": [
    "<h1> Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7ad8d-8080-43e8-996f-83d4444e5d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 932\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 150\n",
      "  Number of trainable parameters = 120,766,465\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 77/150 00:06 < 00:06, 11.41 it/s, Epoch 5.07/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad8bc7-519b-44d1-b6bb-98b3ab95c38d",
   "metadata": {},
   "source": [
    "<h1> Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2e88a-9647-4c44-a090-693b15b124df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(df_test.map(process_exampleTemp, remove_columns=list(df_test[0].keys())))\n",
    "df_testOrignal['prediction'] = [x[0] for x in pred.predictions[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcb15f-9212-4e82-b73e-553b9c628b7e",
   "metadata": {},
   "source": [
    "<h1> Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1cbaf-6edb-42f9-b9a0-d59c0ca616db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_testOrignal[['ID', 'prediction']].to_csv(file_save+'/v_reg.tsv', index=False, sep=\"\\t\")\n",
    "pd.read_csv(file_save+\"/v_reg.tsv\", sep=\"\\t\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b710f-ef02-4917-9c06-adf6a12ddf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "# model.py add this\n",
    "#------------------------------\n",
    "# class ShibaForRegression(ShibaForTask):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(ShibaForRegression, self).__init__(**kwargs)\n",
    "#         self.regression_layer = torch.nn.Linear(self.shiba_model.config.hidden_size, 1)  # Output is a single continuous value\n",
    "#         self.dropout = torch.nn.Dropout(p=self.shiba_model.config.dropout)\n",
    "#         self.loss = torch.nn.MSELoss()  # Use Mean Squared Error (MSE) as the regression loss\n",
    "\n",
    "#     def forward(self, input_ids: torch.Tensor, labels: Optional[torch.Tensor],\n",
    "#                 attention_mask: torch.Tensor) -> Tuple:\n",
    "#         embeddings = self.shiba_model(input_ids, attention_mask, None)['embeddings']\n",
    "\n",
    "#         regression_input = self.dropout(embeddings[:, 0, :])  # Apply dropout to the input\n",
    "#         regression_output = self.regression_layer(regression_input)  # Assuming you want to predict a single value for the entire sequence\n",
    "\n",
    "#         output = {\n",
    "#             'embeddings': embeddings,\n",
    "#             'regression_output': regression_output\n",
    "#         }\n",
    "\n",
    "#         if labels is not None:\n",
    "#             loss = self.loss(regression_output, labels.view(-1, 1))  # Use Mean Squared Error (MSE) as the regression loss\n",
    "#             output['loss'] = loss\n",
    "\n",
    "#         return output.get('loss', None), output['regression_output'], output['embeddings']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece11ec5-11f0-4a47-a260-79c063823ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2148ba5-e5cb-44a2-a6ad-b527d3aa20b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
