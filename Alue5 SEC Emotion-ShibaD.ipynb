{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ae0334-5eeb-4062-b380-a42acd260608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install shiba shiba-model evaluate datasets wandb arabert  accelerate -U nltk torchmetrics==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78bcf5d-aded-4be5-b93a-ed7580614867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import HfArgumentParser, Trainer, EvalPrediction\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "from shiba import ShibaForClassificationD, CodepointTokenizer\n",
    "from training.helpers import DataArguments, get_base_shiba_state_dict,get_model_hyperparams, ShibaClassificationArgs, \\\n",
    "    ClassificationDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98999aa3-2e36-49cc-b380-3a2cd7c5687b",
   "metadata": {},
   "source": [
    "<h1> Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f7f657-65f9-4d7a-b3af-54c8ecf28d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '../checkpoint-611960.pt'\n",
    "seg_enable = True\n",
    "bert_model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "apply_farasa=False\n",
    "file_save = 'SEC'\n",
    "batch = 8\n",
    "drop_it = 0.3\n",
    "num_train_epochs = 10\n",
    "conf = str(drop_it)+\"_\"+str(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4eab0-278a-44a7-b6b0-09c3dadb4e9b",
   "metadata": {},
   "source": [
    "# Pre-Process data ( if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952ccbe-9bfa-43d2-8036-dd3d64f280e3",
   "metadata": {},
   "source": [
    "<h1> Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997544c5-69cb-402b-a67f-3aeb2844e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics = lambda p: {'jaccard_similarity': jaccard_score(p.predictions[0] > 0.5, p.label_ids, average=\"macro\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504ee3ad-2401-43e6-a822-9bf38a76faab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_info()\n",
    "device = \"cuda\"\n",
    "parser = HfArgumentParser((ShibaClassificationArgs, DataArguments))\n",
    "\n",
    "\n",
    "df_testOrignal = pd.read_csv(\"data/emotion_no_labels_v1.0.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_train = pd.read_csv(\"data/2018-E-c-Ar-train.txt\", sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"data/2018-E-c-Ar-dev.txt\", sep=\"\\t\")\n",
    "df_test = df_testOrignal.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5160895d-c709-4beb-a5b6-6bd88ed87958",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.columns[2:]].iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56919a18-ba16-45d7-a4cc-4a3c17b2a27d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism',\n",
      "       'pessimism', 'sadness', 'surprise', 'trust'],\n",
      "      dtype='object')\n",
      "{'dropout': 0.1, 'deep_transformer_stack_layers': 12, 'local_attention_window': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and using base shiba states from ../checkpoint-611960.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "transformers.logging.set_verbosity_info()\n",
    "device = \"cuda\"\n",
    "parser = HfArgumentParser((ShibaClassificationArgs, DataArguments))\n",
    "df_testOrignal = pd.read_csv(\"data/emotion_no_labels_v1.0.tsv\", sep=\"\\t\")\n",
    "df_train = pd.read_csv(\"data/2018-E-c-Ar-train.txt\", sep=\"\\t\")\n",
    "df_dev = pd.read_csv(\"data/2018-E-c-Ar-dev.txt\", sep=\"\\t\")\n",
    "df_test = df_testOrignal.copy()\n",
    "if seg_enable:\n",
    "    from arabert.preprocess import ArabertPreprocessor\n",
    "    arabert_prep = ArabertPreprocessor(model_name=bert_model_name,apply_farasa_segmentation=apply_farasa)\n",
    "    df_train['Tweet'] =  df_train['Tweet'].apply(arabert_prep.preprocess)\n",
    "    df_dev['Tweet'] =  df_dev['Tweet'].apply(arabert_prep.preprocess)\n",
    "    df_test['Tweet'] =  df_test['Tweet'].apply(arabert_prep.preprocess)\n",
    "prediction_label = df_train.columns[2:]\n",
    "print(prediction_label)\n",
    "def process_example(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Tweet'])['input_ids'][:model.config.max_length],\n",
    "        'labels': [float(example[label]) for label in prediction_label]\n",
    "    }\n",
    "def process_exampleTemp(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Tweet'])['input_ids'][:model.config.max_length],\n",
    "        'labels': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    }\n",
    "\n",
    "df_train = Dataset.from_pandas(df_train)\n",
    "df_dev = Dataset.from_pandas(df_dev)\n",
    "df_test = Dataset.from_pandas(df_test)\n",
    "tokenizer = CodepointTokenizer()\n",
    "model_hyperparams = {'dropout': 0.1, 'deep_transformer_stack_layers': 12, 'local_attention_window': 128}\n",
    "print(model_hyperparams)\n",
    "model = ShibaForClassificationD(vocab_size=len(prediction_label), **model_hyperparams)\n",
    "data_collator = ClassificationDataCollator()\n",
    "print('Loading and using base shiba states from', model_path)\n",
    "checkpoint_state_dict = torch.load(model_path)\n",
    "model.shiba_model.load_state_dict(get_base_shiba_state_dict(checkpoint_state_dict))\n",
    "training_args = ShibaClassificationArgs(\n",
    "    per_device_eval_batch_size=batch,\n",
    "    per_device_train_batch_size=batch,\n",
    "    data_seed=42,\n",
    "    seed=42,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    do_train=True,\n",
    "    dropout=drop_it,\n",
    "    eval_accumulation_steps=None,\n",
    "    eval_delay=0,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    output_dir=\"fine_result\",\n",
    "    prediction_loss_only=False,\n",
    "    report_to=[],\n",
    "    run_name=\"fine_result\",\n",
    "    save_strategy='no',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "223f23f2-62cd-4232-a053-3ad59a45b026",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism',\n",
       "       'pessimism', 'sadness', 'surprise', 'trust'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a5ded89-53b0-4315-891b-de6a2de5482d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ebf9e229d542898d23235260426682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b9d3fa2ea84db5b93ced6f72aaa5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/585 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(all_data)\n",
    "trainer = Trainer(model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=df_train.map(process_example, remove_columns=list(df_train[0].keys())),\n",
    "                eval_dataset=df_dev.map(process_example, remove_columns=list(df_dev[0].keys())),\n",
    "                compute_metrics=compute_metrics,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dc85330-6635-407f-800c-5e2b960a0caa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,278\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2,850\n",
      "  Number of trainable parameters = 120,774,155\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2850' max='2850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2850/2850 05:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Jaccard Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.536400</td>\n",
       "      <td>0.476198</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.449300</td>\n",
       "      <td>0.420427</td>\n",
       "      <td>0.024945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.394800</td>\n",
       "      <td>0.390726</td>\n",
       "      <td>0.188934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.364000</td>\n",
       "      <td>0.377598</td>\n",
       "      <td>0.207948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.362991</td>\n",
       "      <td>0.220805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>0.351737</td>\n",
       "      <td>0.255473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>0.358053</td>\n",
       "      <td>0.272933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.277800</td>\n",
       "      <td>0.352903</td>\n",
       "      <td>0.275865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.348103</td>\n",
       "      <td>0.283563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.243700</td>\n",
       "      <td>0.347885</td>\n",
       "      <td>0.277567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.253100</td>\n",
       "      <td>0.349339</td>\n",
       "      <td>0.318571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>0.345124</td>\n",
       "      <td>0.300477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.218300</td>\n",
       "      <td>0.347747</td>\n",
       "      <td>0.303983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.347054</td>\n",
       "      <td>0.323705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.191100</td>\n",
       "      <td>0.347970</td>\n",
       "      <td>0.318393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.348656</td>\n",
       "      <td>0.308306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.351055</td>\n",
       "      <td>0.320592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.357290</td>\n",
       "      <td>0.329608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.168100</td>\n",
       "      <td>0.357832</td>\n",
       "      <td>0.321419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>0.323661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.364281</td>\n",
       "      <td>0.331556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.362709</td>\n",
       "      <td>0.329012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.156800</td>\n",
       "      <td>0.369018</td>\n",
       "      <td>0.315159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.365270</td>\n",
       "      <td>0.318908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.367185</td>\n",
       "      <td>0.321643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.366694</td>\n",
       "      <td>0.324738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.367241</td>\n",
       "      <td>0.322837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.367886</td>\n",
       "      <td>0.326765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "821ebf36-63e2-4e25-9c7d-a2d5aae31a17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_exampleTemp at 0x7f3001171c60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f346770271348388b749909d0a508f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(df_test.map(process_exampleTemp, remove_columns=list(df_test[0].keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd7162d-07d9-4b7b-904c-1f4c83dae8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17470</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16262</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13597</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>6877</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>10056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>16364</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5406</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>8326</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  anger  anticipation  disgust  fear  joy  love  optimism  \\\n",
       "0    17439      0             0        0     0    0     0         0   \n",
       "1    10196      0             0        0     0    0     0         0   \n",
       "2    17470      1             0        0     0    0     0         0   \n",
       "3    16262      1             0        0     0    0     0         0   \n",
       "4    13597      1             0        0     0    0     0         0   \n",
       "..     ...    ...           ...      ...   ...  ...   ...       ...   \n",
       "995   6877      1             0        1     0    0     0         0   \n",
       "996  10056      0             0        0     1    0     0         0   \n",
       "997  16364      0             0        0     0    0     0         0   \n",
       "998   5406      1             0        0     0    0     0         0   \n",
       "999   8326      0             0        0     1    0     0         0   \n",
       "\n",
       "     pessimism  sadness  surprise  trust  \n",
       "0            0        1         0      0  \n",
       "1            1        1         0      0  \n",
       "2            1        1         0      0  \n",
       "3            0        0         0      0  \n",
       "4            0        1         0      0  \n",
       "..         ...      ...       ...    ...  \n",
       "995          0        0         0      0  \n",
       "996          0        0         0      0  \n",
       "997          0        0         0      0  \n",
       "998          0        0         0      0  \n",
       "999          0        0         0      0  \n",
       "\n",
       "[1000 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_save = pd.DataFrame(data=pred.predictions[0] > 0.5, columns=prediction_label, index=df_testOrignal[\"ID\"]).astype(int)\n",
    "df_save.reset_index(inplace=True)\n",
    "df_save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcb15f-9212-4e82-b73e-553b9c628b7e",
   "metadata": {},
   "source": [
    "<h1> Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17e1cbaf-6edb-42f9-b9a0-d59c0ca616db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEC/E_c.tsv0.3_8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17470</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  anger  anticipation  disgust  fear  joy  love  optimism  pessimism  \\\n",
       "0  17439      0             0        0     0    0     0         0          0   \n",
       "1  10196      0             0        0     0    0     0         0          1   \n",
       "2  17470      1             0        0     0    0     0         0          1   \n",
       "\n",
       "   sadness  surprise  trust  \n",
       "0        1         0      0  \n",
       "1        1         0      0  \n",
       "2        1         0      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_save.to_csv(file_save+'/E_c.tsv'+conf, index=False, sep=\"\\t\")\n",
    "print(file_save+'/E_c.tsv'+conf)\n",
    "pd.read_csv(file_save+'/E_c.tsv'+conf, sep=\"\\t\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce30d77-592b-4698-a3a8-389ce9fdb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ShibaForClassificationD(ShibaForTask):\n",
    "#     def __init__(self, vocab_size: int, **kwargs):\n",
    "#         super(ShibaForClassificationD, self).__init__(**kwargs)\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.config = self.shiba_model.config\n",
    "#         self.config.vocab_size = self.vocab_size\n",
    "#         self.label_layer = torch.nn.Linear(self.shiba_model.config.hidden_size, self.vocab_size)\n",
    "#         self.dropout = torch.nn.Dropout(p=self.shiba_model.config.dropout)\n",
    "\n",
    "#         self.loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "#     def forward(self, input_ids: torch.Tensor, labels: Optional[torch.Tensor],\n",
    "#                 attention_mask: torch.Tensor) -> Tuple:\n",
    "#         cls_embeddings = self.shiba_model(input_ids, attention_mask, None)['embeddings'][:, 0, :]\n",
    "#         class_hidden_states = self.label_layer(self.dropout(cls_embeddings))\n",
    "\n",
    "#         output = {\n",
    "#             'cls_embeddings': cls_embeddings,\n",
    "#             'class_probs': class_hidden_states  # Note: no log_softmax here for BCEWithLogitsLoss\n",
    "#         }\n",
    "\n",
    "#         if labels is not None:\n",
    "#             output['loss'] = self.loss(class_hidden_states, labels)\n",
    "\n",
    "#         return output.get('loss', None), output['class_probs'], output['cls_embeddings']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
