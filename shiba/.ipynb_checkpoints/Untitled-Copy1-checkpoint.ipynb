{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca3533a-6a26-408f-8769-e2cbcaf8db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import HfArgumentParser, Trainer, EvalPrediction, BertForSequenceClassification, AutoTokenizer, \\\n",
    "    DataCollatorWithPadding\n",
    "\n",
    "from shiba.helpers import DataArguments, get_model_hyperparams, ShibaClassificationArgs, \\\n",
    "    ClassificationDataCollator, get_base_shiba_state_dict\n",
    "from shiba import ShibaForClassification, CodepointTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pickle\n",
    "metric = evaluate.load(\"f1\")\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495fceab-21f5-48e8-84a7-774f35e5398d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_args :  ShibaClassificationArgs(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deep_transformer_stack_layers=12,\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=False,\n",
      "dropout=0.1,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=300,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_only_model=False,\n",
      "local_attention_window=128,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=fine_result/runs/Sep13_14-17-52_16f4bb6a27c5,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "masking_type=rand_span,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=20,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=fine_result,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=12,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "pretrained_bert=None,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=../checkpoint-63528.pt,\n",
      "run_name=fine_result,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trials=2,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.025,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "categories :  {0: 'OFF', 1: 'NOT_OFF'}\n",
      "id_by_category :  {'OFF': 0, 'NOT_OFF': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_556/1113248289.py:17: ParserWarning: Falling back to the 'python' engine because ord(quotechar) > 127, meaning the quotechar is larger than one byte, and the 'c' engine does not support such quotechars; you can avoid this warning by specifying engine='python'.\n",
      "  df_train = pd.read_csv(\"../data/OSACT2020-sharedTask-train.txt\", sep=\"\\t\", quotechar='▁', header=None, names=[\"Feed\", \"Sentiment\", \"offensive\"])[['Feed','Sentiment']]\n",
      "/tmp/ipykernel_556/1113248289.py:18: ParserWarning: Falling back to the 'python' engine because ord(quotechar) > 127, meaning the quotechar is larger than one byte, and the 'c' engine does not support such quotechars; you can avoid this warning by specifying engine='python'.\n",
      "  df_dev = pd.read_csv(\"../data/OSACT2020-sharedTask-dev.txt\", sep=\"\\t\", quotechar='▁', header=None, names=[\"Feed\", \"Sentiment\", \"offensive\"])[['Feed','Sentiment']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and using base shiba states from ../checkpoint-63528.pt\n"
     ]
    }
   ],
   "source": [
    "transformers.logging.set_verbosity_info()\n",
    "device = \"cuda\"\n",
    "parser = HfArgumentParser((ShibaClassificationArgs, DataArguments))\n",
    "\n",
    "# training_args, data_args = parser.parse_args_into_dataclasses()\n",
    "with open('training_args.pkl', 'rb') as f:\n",
    "    training_args = pickle.load(f)\n",
    "\n",
    "print(\"training_args : \", training_args)\n",
    "training_args.logging_dir = training_args.output_dir\n",
    "training_args.save_steps = 50\n",
    "training_args.logging_steps = 50\n",
    "training_args.eval_steps = 50\n",
    "training_args.num_train_epochs = 10\n",
    "training_args.per_device_train_batch_size = 64\n",
    "training_args.per_device_eval_batch_size = 64\n",
    "df_train = pd.read_csv(\"../data/OSACT2020-sharedTask-train.txt\", sep=\"\\t\", quotechar='▁', header=None, names=[\"Feed\", \"Sentiment\", \"offensive\"])[['Feed','Sentiment']]\n",
    "df_dev = pd.read_csv(\"../data/OSACT2020-sharedTask-dev.txt\", sep=\"\\t\", quotechar='▁', header=None, names=[\"Feed\", \"Sentiment\", \"offensive\"])[['Feed','Sentiment']]\n",
    "# df_test = pd.read_csv(\"../data/tweets_v1.0.txt\", sep=\"\\t\", quotechar='▁', header=None, names=[\"Feed\"])\n",
    "# categories = {idx: cat_name for idx, cat_name in enumerate({x['Sentiment'] for x in all_data})}\n",
    "categories = {idx: cat_name for idx, cat_name in enumerate(set(df_train['Sentiment']))}\n",
    "id_by_category = {val: key for key, val in categories.items()}\n",
    "\n",
    "print(\"categories : \", categories)\n",
    "print(\"id_by_category : \", id_by_category)\n",
    "\n",
    "tokenizer = CodepointTokenizer()\n",
    "model_hyperparams = get_model_hyperparams(training_args)\n",
    "model = ShibaForClassification(vocab_size=len(categories), **model_hyperparams).to(device)\n",
    "data_collator = ClassificationDataCollator()\n",
    "\n",
    "if training_args.resume_from_checkpoint:\n",
    "    print('Loading and using base shiba states from', training_args.resume_from_checkpoint)\n",
    "    checkpoint_state_dict = torch.load(training_args.resume_from_checkpoint)\n",
    "    model.shiba_model.load_state_dict(get_base_shiba_state_dict(checkpoint_state_dict))\n",
    "\n",
    "def process_example(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Feed'])['input_ids'][:model.config.max_length],\n",
    "        'labels': id_by_category[example['Sentiment']]\n",
    "    }\n",
    "def process_exampleTemp(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Feed'])['input_ids'][:model.config.max_length],\n",
    "    }\n",
    "def compute_metrics(pred: EvalPrediction) -> Dict:\n",
    "    try:\n",
    "        # Convert predictions and labels to PyTorch tensors\n",
    "        # label_probs = torch.tensor(pred.predictions)\n",
    "        label_probs, embeddings = pred.predictions\n",
    "        labels = torch.tensor(pred.label_ids)\n",
    "        label_probs = torch.exp(torch.tensor(label_probs))  # undo the log in log softmax, get indices\n",
    "\n",
    "        # # Compute accuracy\n",
    "        # accuracy = torchmetrics.functional.accuracy(label_probs, labels, num_classes=len(categories))\n",
    "\n",
    "        # Compute F1 score\n",
    "        f1_score = torchmetrics.functional.f1(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # Compute recall\n",
    "        recall = torchmetrics.functional.recall(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # Compute precision\n",
    "        precision = torchmetrics.functional.precision(label_probs, labels, average='macro', num_classes=len(categories))\n",
    "\n",
    "        # print(\"label_probs : \", label_probs, \" labels : \", labels)\n",
    "\n",
    "        metrics = {\n",
    "            # 'accuracy': accuracy.item(),\n",
    "            'f1_score': f1_score.item(),\n",
    "            'recall': recall.item(),\n",
    "            'precision': precision.item()\n",
    "        }\n",
    "\n",
    "        # print(\"metrics : \", metrics)\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        return metrics\n",
    "    except:\n",
    "        print(\"pred : \", pred)\n",
    "        print(\"pred.predictions : \", pred.predictions)\n",
    "        print(\"label_probs : \", label_probs)\n",
    "        print(\"label_probs.size : \", label_probs.size())\n",
    "        print(\"labels : \", labels)\n",
    "        print(\"labels.size() : \", labels.size())\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "os.environ['WANDB_PROJECT'] = 'shiba'\n",
    "df_train = Dataset.from_pandas(df_train)\n",
    "df_dev = Dataset.from_pandas(df_dev)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793d8556-0aeb-4842-a616-e046defc5ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bcfe3e6-95ab-4c8a-bad2-33d41b81d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1 = df_train[:6000]\n",
    "df_test = df_train[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71951231-19c3-4bb3-90c7-58a5ee0108c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "babb6d8d-7256-4441-b19b-ea710ec6a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "df_train1 = Dataset.from_dict(df_train1)\n",
    "df_test = Dataset.from_dict(df_test)\n",
    "\n",
    "print(type(df_train1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc102e6-b165-4fe3-b669-81112292e35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e601d8643914573a0b05551e50c67f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1958366287cc4f5385b80accf2aaf9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'ShibaClassificationArgs' object has no attribute 'deepspeed_plugin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print(all_data)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_train1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_train1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_dev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_dev\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:345\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3899\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3894\u001b[0m gradient_accumulation_plugin \u001b[38;5;241m=\u001b[39m GradientAccumulationPlugin(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgrad_acc_kwargs)\n\u001b[1;32m   3896\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m Accelerator(\n\u001b[1;32m   3898\u001b[0m     dispatch_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdispatch_batches,\n\u001b[0;32m-> 3899\u001b[0m     deepspeed_plugin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepspeed_plugin\u001b[49m,\n\u001b[1;32m   3900\u001b[0m     gradient_accumulation_plugin\u001b[38;5;241m=\u001b[39mgradient_accumulation_plugin,\n\u001b[1;32m   3901\u001b[0m )\n\u001b[1;32m   3903\u001b[0m \u001b[38;5;66;03m# deepspeed and accelerate flags covering both trainer args and accelerate launcher\u001b[39;00m\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepspeed_plugin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ShibaClassificationArgs' object has no attribute 'deepspeed_plugin'"
     ]
    }
   ],
   "source": [
    "# print(all_data)\n",
    "trainer = Trainer(model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=df_train1.map(process_example, remove_columns=list(df_train1[0].keys())),\n",
    "                eval_dataset=df_dev.map(process_example, remove_columns=list(df_dev[0].keys())),\n",
    "                compute_metrics=compute_metrics\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d561d50-2b22-4fbd-8d3d-abec3228bba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "training = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddc48baf-1d31-4963-ae71-b76d8b5ef024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testtmp = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca35f3d-3e46-4d53-9ae3-068538bc2eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99322b4a65fc4268a279d2cae4f5956a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def process_exampleTemp(example: Dict) -> Dict:\n",
    "    return {\n",
    "        'input_ids': tokenizer.encode(example['Feed'])['input_ids'][:model.config.max_length],\n",
    "        'labels': 0\n",
    "    }\n",
    "what = trainer.predict(df_testtmp.map(process_exampleTemp, remove_columns=list(df_testtmp[0].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0aec2d-33d1-4217-ae9d-eb21d7c1d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf64224e-5557-48a6-80a3-a28a66fee4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_test['Offensive'] = [{0: 'HS', 1: 'NOT_HS'}[x] for x in np.argmax(what.predictions[0], axis=1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8059e12-5a32-47dd-a561-0bbac4a13ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_563/3665250243.py:2: ParserWarning: Falling back to the 'python' engine because ord(quotechar) > 127, meaning the quotechar is larger than one byte, and the 'c' engine does not support such quotechars; you can avoid this warning by specifying engine='python'.\n",
      "  pd.read_csv(\"tweets_v2.0_offensive.csv\", sep=\"\\t\", quotechar='▁')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feed</th>\n",
       "      <th>Offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>أود أن أعلمكم أن التعليق المنشور هنا باسم نور ...</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>مافيه فرق بين احمد جبريل والعاهره المستأجره</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اذا نطق السفية فلا تجبة لانة سفية وقليل الادب ...</td>\n",
       "      <td>HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اعتقد حضرتك تدعو لمؤتمر دولى للحوار للسلمي مع ...</td>\n",
       "      <td>HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>يسرني في المركز الموريتاني لقياس الراي العام ا...</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>رحت ع الجامعه لقيتها مسكرة  -نمزيت</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>تجميع وليس تصنيع، وإعادة تصدير وليس تصدير، اقت...</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>احلى اشي لما تكون مقتنع بشغلك وعملك هيك ما بتح...</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>لو عملت مقارنة بسيطة بين داعش والحشد الطائفي ل...</td>\n",
       "      <td>HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>تحياتى للكاتبة ...من اين حصلتي علي إحصائية الت...</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Feed Offensive\n",
       "0    أود أن أعلمكم أن التعليق المنشور هنا باسم نور ...    NOT_HS\n",
       "1          مافيه فرق بين احمد جبريل والعاهره المستأجره    NOT_HS\n",
       "2    اذا نطق السفية فلا تجبة لانة سفية وقليل الادب ...        HS\n",
       "3    اعتقد حضرتك تدعو لمؤتمر دولى للحوار للسلمي مع ...        HS\n",
       "4    يسرني في المركز الموريتاني لقياس الراي العام ا...    NOT_HS\n",
       "..                                                 ...       ...\n",
       "995                 رحت ع الجامعه لقيتها مسكرة  -نمزيت    NOT_HS\n",
       "996  تجميع وليس تصنيع، وإعادة تصدير وليس تصدير، اقت...    NOT_HS\n",
       "997  احلى اشي لما تكون مقتنع بشغلك وعملك هيك ما بتح...    NOT_HS\n",
       "998  لو عملت مقارنة بسيطة بين داعش والحشد الطائفي ل...        HS\n",
       "999  تحياتى للكاتبة ...من اين حصلتي علي إحصائية الت...    NOT_HS\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.to_csv('tweets_v2.0_offensive.csv',sep=\"\\t\", quotechar='▁', index=False)\n",
    "pd.read_csv(\"tweets_v2.0_offensive.csv\", sep=\"\\t\", quotechar='▁')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933769ff-1b9a-4879-b9bb-bff9d8ef8ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
